## Описание решения

1. Выбираем `kind` как легковесное решение для создания мультинодового кластера в локальной среде (minikube неудобно, managed кластеры в облаке - очевидно требуют денег).
2. Удовлетворяем требованию "5 нод, 3 зоны". [Описываем](cluster/cluster.yaml) мультинодовую конфигурацию `kind`, назначая метки, символизирующие зону, и метки для размещения `ingress`-контроллера.
3. В качестве веб-приложения берём сэмпл из туториалов GCP (явных ограничений в задании на это нет).
4. Удовлетворяем требование "Приложению нужно 5-10 секунд на инициализацию" с помощью [Readiness Probe](app/deployment.yaml). Первая проба будет отправлена спустя 5 секунд после старта, если она будет неуспешна, то спустя ещё 5 секунд будет отправлена вторая. На случай аномалий оставляем количество возможных проваленных проб равным 3.
5. Удовлетворяем требованию "4 пода справляются с пиковой нагрузкой" и "приложение имеет дневной цикл нагрузки" - с этим справляется [Horizontal Pod Autoscaler](app/hpa.yaml). Так как явно заданных метрик у нас нет, а "потребление памяти всегда ровное в районе `128Mi`", то логично скейлить, когда нагрузка по CPU будет расти. (Число 75% взято с потолка.)
6. Удовлетворяем требованиям по CPU/memory (заодно требованию "эффективное использование ресурсов"). По условию, потребление памяти всегда ровное, его задаём с помощью `resourse.limits` и `resource.requests`. С CPU сложнее. Как удовлетворить требование "у приложения после старта высокое потребление, а потом ровное"? Vertical Pod Autoscaler не подходит, так как рестартует под (либо надо применять новые значения ресурсов вручную), т.е. у нас каждый раз будет повышенная нагрузка на CPU, и таким образом, мы ничего не сэкономим. Однако, если мы зададим CPU request, то под будет иметь класс QoS burstable, и, при необходимости (и наличии ресурсов), будет использовать больше CPU. (А если свободных ресурсов не будет, то будет троттлиться, и под будет медленнее отвечать на первые запросы.) Как ещё можно сделать лучше? Добавить `LimitRange` на контейнер и `ResourceQuota` на namespace, тогда предельное потребление ресурсов приложением будет более предсказуемо.
7. Удовлетворяем требованиям отказоустойчивости. С помощью [topologySpreadConstraints](app/deployment.yaml) размазываем поды по зонам. С помощью [Pod Disruption Budget](app/pdb.yaml) страхуемся от действий администраторов кластера.
8. Очевидности:
    * про стейт приложения ничего не сказано, считаем его stateless, а значит - `Deployment` (не `StatefulSet`, не `DaemonSet`);
    * как следствие, забываем про `PV`, `PVC`, `StorageClass`, ...;
    * *веб*-приложение - значит, публикуем через `service`, делаем доступ извне через `ingress`;
    * отдельное пространство имён.
